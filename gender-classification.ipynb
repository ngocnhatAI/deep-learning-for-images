{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78157452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c66003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "def load_images_from_folder(folder, label=None):\n",
    "    images = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        if filename.endswith('.jpg'):\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "                images.append(img)\n",
    "                if label is not None:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    ids.append(filename.split('.')[0])\n",
    "    if label is not None:\n",
    "        return np.array(images), np.array(labels)\n",
    "    else:\n",
    "        return np.array(images), ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d93ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MALE = 0, FEMALE = 1\n",
    "\n",
    "train_female_imgs, train_female_labels = load_images_from_folder('.\\\\data\\\\gender-classification\\\\Train\\\\Female', 1)\n",
    "train_male_imgs, train_male_labels = load_images_from_folder('.\\\\data\\\\gender-classification\\\\Train\\\\Male', 0)\n",
    "X_train = np.concatenate([train_female_imgs, train_male_imgs], axis=0)          # Shape: (8000, 224, 224, 3)\n",
    "y_train = np.concatenate([train_female_labels, train_male_labels], axis=0)      # Shape: (8000,)\n",
    "\n",
    "val_female_imgs, val_female_labels = load_images_from_folder('.\\\\data\\\\gender-classification\\\\Val\\\\Female', 1)\n",
    "val_male_imgs, val_male_labels = load_images_from_folder('.\\\\data\\\\gender-classification\\\\Val\\\\Male', 0)\n",
    "X_val = np.concatenate([val_female_imgs, val_male_imgs], axis=0)                # Shape: (1000, 224, 224, 3)\n",
    "y_val = np.concatenate([val_female_labels, val_male_labels], axis=0)            # Shape: (1000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(X, y, batch_size=32, shuffle=True, num_workers = 0):\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    # Change shape from (N, H, W, C) to (N, C, H, W) and scale pixel values to [0, 1]\n",
    "    X_tensor = torch.from_numpy(X).permute(0, 3, 1, 2).float() / 255.0\n",
    "    y_tensor = torch.from_numpy(y).long()\n",
    "\n",
    "    # Normalize using ImageNet statistics\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    X_tensor = (X_tensor - mean) / std\n",
    "\n",
    "    # Wrap the tensors in a Dataset and create a DataLoader\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, \n",
    "                            num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3bfc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_dataloader(X_train, y_train)\n",
    "val_loader = get_dataloader(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f77c7526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape)  # (batch_size, 3, H, W)\n",
    "print(labels.shape)  # (batch_size,)\n",
    "print(images[0].shape)\n",
    "print(labels[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e391248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\Admin/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:01<00:00, 30.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained ResNet-18 model\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Modify the final fully connected layer for binary classification (male/female)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0397019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1289\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# TRAIN IN ONE EPOCH FOR TESTING\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2e1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set: 99.15%\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on validation set: {100 * correct / total:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
